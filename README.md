# What is Artificial Intelligence (AI)?

**Artificial intelligence (AI)** is a technology that enables computers and machines to simulate human learning, comprehension, problem solving, decision making, creativity and autonomy. Applications and devices equipped with AI can see and identify objects. They can understand and respond to human language. They can learn from new information and experience. They can make detailed recommendations to users and experts. They can act independently, replacing the need for human intelligence or intervention (a classic example being a self-driving car). But in 2024, most AI researchers and practitioners—and most AI-related headlines—are focused on breakthroughs in generative AI (gen AI), a technology that can create original text, images, video and other content. To fully understand generative AI, it’s important to first understand the technologies on which generative AI tools are built: machine learning (ML) and deep learning.

---

## Machine learning

**Machine learning** Within AI, we have machine learning, which involves creating models by training an algorithm to make predictions or decisions based on data. It encompasses a broad range of techniques that enable computers to learn from and make inferences based on data without being explicitly programmed for specific tasks. There are many types of machine learning techniques or algorithms, including linear regression, logistic regression, decision trees, random forest, support vector machines (SVMs), k-nearest neighbor (KNN), clustering and more. Each of these approaches is suited to different kinds of problems and data.

But one of the most popular types of machine learning algorithm is called a neural network (or artificial neural network). Neural networks are modeled after the human brain's structure and function. A neural network consists of interconnected layers of nodes (analogous to neurons) that work together to process and analyze complex data. Neural networks are well suited to tasks that involve identifying complex patterns and relationships in large amounts of data.

The simplest form of machine learning is called supervised learning, which involves the use of labeled data sets to train algorithms to classify data or predict outcomes accurately. In supervised learning, humans pair each training example with an output label. The goal is for the model to learn the mapping between inputs and outputs in the training data, so it can predict the labels of new, unseen data.


### Deep learning

**Deep learning** is a subset of machine learning that utilizes multilayered neural networks, known as **deep neural networks**, which more closely simulate the complex decision-making capabilities of the human brain. Deep neural networks comprise an **input layer**, multiple **hidden layers** (typically three or more, but often hundreds), and an **output layer**. This architecture differs from neural networks in classic machine learning, which generally include only one or two hidden layers.

These multiple layers enable [unsupervised learning](https://www.ibm.com/topics/unsupervised-learning): they can autonomously extract features from large, unlabeled, and unstructured datasets and make independent predictions about the data's content. Since deep learning minimizes the need for human intervention, it supports machine learning on a massive scale. Deep learning is particularly effective for tasks like [natural language processing (NLP)](https://www.ibm.com/topics/natural-language-processing) and [computer vision](https://www.ibm.com/topics/computer-vision), where fast and accurate pattern recognition in vast datasets is essential. Today, some form of deep learning drives most of the AI applications we encounter.

Deep learning also enables:

- [Semi-supervised learning](https://www.ibm.com/topics/semi-supervised-learning), which combines supervised and unsupervised learning by using both labeled and unlabeled data to train AI models for classification and regression tasks.

- [Self-supervised learning](https://www.ibm.com/topics/self-supervised-learning), which generates implicit labels from unstructured data, enabling the model to learn without the need for extensive human-labeled datasets.


---

## Types of Artificial Intelligence

AI can be categorized based on its development stages or functional capabilities:

1. **Reactive Machines:** Limited AI that responds to stimuli without learning from past data (e.g., IBM’s Deep Blue).
2. **Limited Memory:** Most modern AI systems; they learn and improve through new data (e.g., deep learning).
3. **Theory of Mind:** Hypothetical AI that could understand and react to human emotions.
4. **Self-Aware AI:** A theoretical AI with human-like consciousness, which does not yet exist.

Another categorization is **Artificial Narrow Intelligence (ANI)**, AI specialized in a single task (e.g., Google Search). Future possibilities include **Artificial General Intelligence (AGI)** and **Artificial Superintelligence (ASI)**, where AI could match or exceed human capabilities, though these do not currently exist.

---

## Artificial Intelligence Training Models

AI training often relies on **training data**:

1. **Supervised Learning:** Maps specific inputs to outputs using labeled data (e.g., recognizing images of cats).
2. **Unsupervised Learning:** Finds patterns in unlabeled data, useful for descriptive modeling.
3. **Semi-Supervised Learning:** Combines labeled and unlabeled data for improved results.
4. **Reinforcement Learning:** An agent learns by trial and error, receiving feedback to improve performance.

---

## Common Types of Artificial Neural Networks

Neural networks, loosely modeled on the human brain, are a primary AI architecture. Here are some commonly used types:

- **Feedforward Neural Networks (FF):** Data flows one-way through layers; often paired with **backpropagation** for error correction.
- **Recurrent Neural Networks (RNN):** Uses memory to process time-sequence data, ideal for language processing.
- **Long/Short-Term Memory (LSTM):** An advanced RNN type with memory cells for long-term dependencies, useful in speech recognition.
- **Convolutional Neural Networks (CNN):** Common in image recognition, processes images through convolution and pooling layers to capture details.
- **Generative Adversarial Networks (GAN):** Two competing networks (generator and discriminator) that improve output accuracy, used for creating realistic images and art.

---

## Benefits of AI

- **Automation:** AI can independently perform workflows and processes, like automating cybersecurity tasks.
- **Reduced Human Error:** Consistent processes eliminate manual errors.
- **Elimination of Repetitive Tasks:** Frees up human resources for high-impact work.
- **Speed and Accuracy:** AI quickly processes and finds patterns in vast amounts of data.
- **Infinite Availability:** AI operates continuously, unaffected by human limitations.
- **Accelerated Research and Development:** Enables breakthroughs, such as in pharmaceuticals or genomics.

## Solve Your Business Challenges with Google Cloud

**New customers get $300 in free credits on Google Cloud.**  
* [Get Started](#)  
* [Sign Up for Newsletters](#)

---

## Applications and Use Cases for Artificial Intelligence

- **Speech Recognition:** Converts spoken language to text.
- **Image Recognition:** Identifies and categorizes elements in an image.
- **Translation:** Translates between languages.
- **Predictive Modeling:** Forecasts specific outcomes based on data.
- **Data Analytics:** Discovers patterns for business intelligence.
- **Cybersecurity:** Detects and responds to cyber threats autonomously.

--- 

